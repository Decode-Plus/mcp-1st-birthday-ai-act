#!/usr/bin/env python3
"""
EU AI Act Compliance Agent - Gradio UI
Interactive web interface for EU AI Act compliance assessment
With MCP tool call visualization and multi-model support
"""

import gradio as gr
from gradio import ChatMessage
import requests
import json
import os
import threading
from pathlib import Path
from typing import List, Generator, Optional
from dotenv import load_dotenv

# Load environment variables from root .env file
ROOT_DIR = Path(__file__).parent.parent.parent.parent  # Go up from src -> eu-ai-act-agent -> apps -> root
load_dotenv(ROOT_DIR / ".env")

# API Configuration
API_URL = os.getenv("API_URL", "http://localhost:3001")
PUBLIC_URL = os.getenv("PUBLIC_URL", "")  # HF Spaces public URL (empty for local dev)
API_TIMEOUT = 600  # seconds - increased for long-running compliance assessments

def get_mcp_url() -> str:
    """Get the MCP server URL based on environment"""
    if PUBLIC_URL:
        # Production: MCP is on the same server via chatgpt_app.py
        return f"{PUBLIC_URL.rstrip('/')}/gradio_api/mcp/"
    return ""

# Model Configuration
AVAILABLE_MODELS = {
    "gpt-oss": {
        "name": "üÜì GPT-OSS 20B (Modal - FREE)",
        "api_key_env": "MODAL_ENDPOINT_URL",
        "description": "Free OpenAI GPT-OSS 20B model hosted on Modal.com - No API key required! ‚ö†Ô∏è May take up to 60s to start responding (cold start). For faster responses and better precision, use another model with your API key."
    },
    "claude-4.5": {
        "name": "Claude 4.5 Sonnet (Anthropic)",
        "api_key_env": "ANTHROPIC_API_KEY",
        "description": "Anthropic's latest Claude Sonnet model"
    },
    "claude-opus": {
        "name": "Claude Opus 4 (Anthropic)",
        "api_key_env": "ANTHROPIC_API_KEY",
        "description": "Anthropic's most powerful Claude model"
    },
    "gemini-3": {
        "name": "Gemini 3 Pro (Google)",
        "api_key_env": "GOOGLE_GENERATIVE_AI_API_KEY",
        "description": "Google's advanced reasoning model with thinking"
    },
    "gpt-5": {
        "name": "GPT-5 (OpenAI)",
        "api_key_env": "OPENAI_API_KEY",
        "description": "OpenAI's most advanced model"
    },
    "grok-4-1": {
        "name": "Grok 4.1 (xAI)",
        "api_key_env": "XAI_API_KEY",
        "description": "xAI's fast reasoning model"
    },
}

# Current model settings (can be updated via UI)
# SECURITY: Store user-provided keys for this session only
# NOTE: API keys are REQUIRED for paid models - GPT-OSS is FREE!
# IMPORTANT: Always default to gpt-oss (FREE model) regardless of env var
# The env var might be set for the API server, but the UI should default to FREE model
current_model_settings = {
    "model": "gpt-oss",  # Always default to FREE GPT-OSS model in UI!
    # User-provided keys (REQUIRED for paid models, optional for GPT-OSS)
    "openai_api_key": "",
    "xai_api_key": "",
    "anthropic_api_key": "",
    "google_api_key": "",  # Google Generative AI API key
    "tavily_api_key": "",  # Required for web research & organization discovery
    "modal_endpoint_url": "https://vasilis--gpt-oss-vllm-inference-serve.modal.run"  # Hardcoded Modal.com endpoint for GPT-OSS (no trailing slash!)
}

# Thread-safe cancellation flag for stopping ongoing requests
class CancellationToken:
    def __init__(self):
        self._cancelled = False
        self._lock = threading.Lock()
        self._response = None
    
    def cancel(self):
        with self._lock:
            self._cancelled = True
            # Close any active response to stop streaming
            if self._response is not None:
                try:
                    self._response.close()
                except:
                    pass
    
    def is_cancelled(self):
        with self._lock:
            return self._cancelled
    
    def set_response(self, response):
        with self._lock:
            self._response = response
    
    def reset(self):
        with self._lock:
            self._cancelled = False
            self._response = None

# Global cancellation token
cancel_token = CancellationToken()

def format_tool_call(tool_name: str, args: dict) -> str:
    """Format a tool call for display"""
    args_str = json.dumps(args, indent=2) if args else "{}"
    return f"""
üîß **MCP Tool Call: `{tool_name}`**

**Arguments:**
```json
{args_str}
```
"""

def format_thinking_section(thinking_text: str, tool_name: str = None) -> str:
    """Format AI thinking/reasoning in a collapsible section"""
    if not thinking_text or not thinking_text.strip():
        return ""
    
    # Clean up the thinking text
    thinking_clean = thinking_text.strip()
    
    # Create a descriptive title based on context
    if tool_name:
        title = f"üß† AI Reasoning (before {tool_name.replace('_', ' ').title()})"
    else:
        title = "üß† AI Reasoning"
    
    return f"""
<details>
<summary>{title}</summary>

*The model's thought process:*

{thinking_clean}

</details>
"""

def format_tool_result(tool_name: str, result) -> str:
    """Format a tool result for display"""
    
    # Special handling for assess_compliance - show generated documentation with full content
    if tool_name == "assess_compliance" and result:
        output = f"\n‚úÖ **Tool Result: `{tool_name}`**\n\n"
        
        # Extract key information
        assessment = result.get("assessment", {})
        metadata = result.get("metadata", {})
        documentation = result.get("documentation", {})
        reasoning = result.get("reasoning", "")
        doc_files = metadata.get("documentationFiles", [])
        
        # Show assessment summary
        if assessment:
            score = assessment.get("overallScore", "N/A")
            risk_level = assessment.get("riskLevel", "N/A")
            gaps = assessment.get("gaps", [])
            recommendations = assessment.get("recommendations", [])
            gaps_count = len(gaps)
            recs_count = len(recommendations)
            
            # Risk level emoji
            risk_emoji = {"CRITICAL": "üî¥", "HIGH": "üü†", "MEDIUM": "üü°", "LOW": "üü¢"}.get(risk_level, "‚ö™")
            
            output += f"""### üìä Compliance Assessment Summary

| Metric | Value |
|--------|-------|
| **Overall Score** | **{score}/100** |
| **Risk Level** | {risk_emoji} **{risk_level}** |
| **Gaps Identified** | {gaps_count} |
| **Recommendations** | {recs_count} |

"""
            
            # Show AI reasoning in collapsible section
            if reasoning:
                output += f"""
<details>
<summary>üß† AI Reasoning & Analysis</summary>

{reasoning}

</details>

"""
            
            # Show gaps summary in collapsible section
            if gaps:
                critical_gaps = [g for g in gaps if g.get("severity") == "CRITICAL"]
                high_gaps = [g for g in gaps if g.get("severity") == "HIGH"]
                
                output += f"""
<details>
<summary>‚ö†Ô∏è Compliance Gaps ({gaps_count} total: {len(critical_gaps)} Critical, {len(high_gaps)} High)</summary>

"""
                # Group by severity
                for severity in ["CRITICAL", "HIGH", "MEDIUM", "LOW"]:
                    severity_gaps = [g for g in gaps if g.get("severity") == severity]
                    if severity_gaps:
                        severity_emoji = {"CRITICAL": "üî¥", "HIGH": "üü†", "MEDIUM": "üü°", "LOW": "üü¢"}.get(severity, "‚ö™")
                        output += f"\n**{severity_emoji} {severity} Priority Gaps:**\n\n"
                        for gap in severity_gaps:
                            output += f"- **{gap.get('category', 'Unknown')}**: {gap.get('description', 'No description')}\n"
                            output += f"  - *Article:* {gap.get('articleReference', 'N/A')} | *Effort:* {gap.get('remediationEffort', 'N/A')}\n"
                
                output += "\n</details>\n\n"
            
            # Show top recommendations in collapsible section
            if recommendations:
                # Sort by priority
                sorted_recs = sorted(recommendations, key=lambda r: r.get("priority", 10))
                top_recs = sorted_recs[:5]
                
                output += f"""
<details>
<summary>üí° Priority Recommendations (Top {len(top_recs)} of {recs_count})</summary>

"""
                for i, rec in enumerate(top_recs, 1):
                    output += f"\n**{i}. {rec.get('title', 'Recommendation')}** (Priority: {rec.get('priority', 'N/A')}/10)\n\n"
                    output += f"{rec.get('description', 'No description')}\n\n"
                    output += f"- *Article:* {rec.get('articleReference', 'N/A')}\n"
                    output += f"- *Estimated Effort:* {rec.get('estimatedEffort', 'N/A')}\n"
                    
                    steps = rec.get("implementationSteps", [])
                    if steps:
                        output += f"- *Implementation Steps:*\n"
                        for step in steps[:3]:  # Show first 3 steps
                            output += f"  1. {step}\n"
                        if len(steps) > 3:
                            output += f"  *(+ {len(steps) - 3} more steps)*\n"
                    output += "\n"
                
                output += "</details>\n\n"
        
        # Show generated documentation content in collapsible sections
        output += "---\n\n### üìÑ Generated EU AI Act Documentation\n\n"
        
        # Map of documentation keys to display info
        doc_display_map = {
            "riskManagementTemplate": {
                "title": "Risk Management System",
                "article": "Article 9",
                "emoji": "‚ö°",
                "description": "Continuous risk identification, analysis, estimation and mitigation process"
            },
            "technicalDocumentation": {
                "title": "Technical Documentation",
                "article": "Article 11 / Annex IV",
                "emoji": "üìã",
                "description": "Comprehensive technical documentation for high-risk AI systems"
            },
            "conformityAssessment": {
                "title": "Conformity Assessment",
                "article": "Article 43",
                "emoji": "‚úÖ",
                "description": "Procedures for conformity assessment of high-risk AI systems"
            },
            "transparencyNotice": {
                "title": "Transparency Notice",
                "article": "Article 50",
                "emoji": "üëÅÔ∏è",
                "description": "Transparency obligations for AI system interactions"
            },
            "qualityManagementSystem": {
                "title": "Quality Management System",
                "article": "Article 17",
                "emoji": "üèÜ",
                "description": "Quality management system for AI system providers"
            },
            "humanOversightProcedure": {
                "title": "Human Oversight Procedure",
                "article": "Article 14",
                "emoji": "üë§",
                "description": "Human oversight measures for high-risk AI systems"
            },
            "dataGovernancePolicy": {
                "title": "Data Governance Policy",
                "article": "Article 10",
                "emoji": "üóÉÔ∏è",
                "description": "Data and data governance practices for training, validation and testing"
            },
            "incidentReportingProcedure": {
                "title": "Incident Reporting Procedure",
                "article": "Article 62",
                "emoji": "üö®",
                "description": "Reporting of serious incidents and malfunctioning"
            },
        }
        
        # Display each documentation template in its own collapsible section
        docs_found = 0
        for doc_key, doc_info in doc_display_map.items():
            doc_content = documentation.get(doc_key)
            if doc_content:
                docs_found += 1
                output += f"""
<details>
<summary>{doc_info['emoji']} **{doc_info['title']}** ‚Äî {doc_info['article']}</summary>

*{doc_info['description']}*

---

{doc_content}

</details>

"""
        
        if docs_found == 0:
            output += "*No documentation templates were generated in this assessment.*\n\n"
        else:
            output += f"\n> ‚ú® **{docs_found} documentation template(s) generated.** Expand each section above to view the full content.\n\n"
            
            # Note about limited templates for speed/cost optimization
            if docs_found < 8:
                output += "> ‚ÑπÔ∏è **Note:** Currently generating **2 core templates** (Risk Management & Technical Documentation) for faster responses and API cost optimization. Additional templates (Conformity Assessment, Transparency Notice, etc.) are planned for future releases.\n\n"
        
        # Show file paths if documents were saved to disk
        if doc_files:
            output += "---\n\n### üíæ Saved Documentation Files\n\n"
            output += "The documentation has also been saved to disk:\n\n"
            
            # Map filenames to EU AI Act articles for context
            article_map = {
                "Risk_Management_System": "Article 9",
                "Technical_Documentation": "Article 11 / Annex IV",
                "Conformity_Assessment": "Article 43",
                "Transparency_Notice": "Article 50",
                "Quality_Management_System": "Article 17",
                "Human_Oversight_Procedure": "Article 14",
                "Data_Governance_Policy": "Article 10",
                "Incident_Reporting_Procedure": "Article 62",
                "Compliance_Assessment_Report": "Full Assessment",
            }
            
            output += "| Document | EU AI Act Reference | File Path |\n"
            output += "|----------|--------------------|-----------|\n"
            
            for file_path in doc_files:
                # Extract filename from path
                filename = file_path.split("/")[-1] if "/" in file_path else file_path
                # Remove .md extension for display name
                display_name = filename.replace(".md", "").replace("_", " ")
                # Remove leading numbers like "01_" or "00_"
                if len(display_name) > 3 and display_name[:2].isdigit() and display_name[2] == " ":
                    display_name = display_name[3:]
                
                # Find article reference
                article_ref = "‚Äî"
                for key, article in article_map.items():
                    if key.lower().replace("_", " ") in display_name.lower():
                        article_ref = article
                        break
                
                output += f"| üìÑ {display_name} | {article_ref} | `{filename}` |\n"
            
            # Show the directory where files are saved
            if doc_files:
                docs_dir = "/".join(doc_files[0].split("/")[:-1])
                output += f"\n**üìÇ Documents Directory:** `{docs_dir}`\n\n"
        
        # Collapsible raw JSON for reference (at the very end)
        result_str = json.dumps(result, indent=2) if result else "null"
        if len(result_str) > 5000:
            result_str = result_str[:5000] + "\n... (truncated)"
        
        output += f"""
---

<details>
<summary>üîç View Raw JSON Response</summary>

```json
{result_str}
```

</details>
"""
        return output
    
    # Default formatting for other tools
    result_str = json.dumps(result, indent=2) if result else "null"
    if len(result_str) > 1500:
        result_str = result_str[:1500] + "\n... (truncated)"
    
    return f"""
‚úÖ **Tool Result: `{tool_name}`**

<details>
<summary>üìã Click to expand result</summary>

```json
{result_str}
```

</details>
"""

def format_thinking_indicator(tool_name: str = None) -> str:
    """Format a thinking/processing indicator"""
    if tool_name:
        # Show specific tool name if available
        tool_display_name = {
            "assess_compliance": "EU AI Act Compliance Assessment",
            "discover_ai_services": "AI Systems Discovery",
            "discover_organization": "Organization Discovery"
        }.get(tool_name, tool_name.replace("_", " ").title())
        
        return f"\n\n‚è≥ **Processing: {tool_display_name}...**\n\n*This may take a moment while the tool analyzes data and generates documentation.*\n"
    return "\n\n‚è≥ **Processing with MCP tools...**\n\n*Please wait while the tools execute...*\n"

def get_api_headers() -> dict:
    """Get headers with model configuration for API requests
    
    SECURITY: Only pass model selection and user-provided API keys.
    API keys are REQUIRED for paid models - GPT-OSS is FREE!
    User must provide their own keys via the Model Settings UI (except for GPT-OSS).
    """
    headers = {"Content-Type": "application/json"}
    
    # Pass model selection - always use the current model setting
    selected_model = current_model_settings.get("model", "gpt-oss")
    headers["X-AI-Model"] = selected_model
    print(f"[Gradio] Sending model to API: {selected_model}")
    
    # Pass user-provided API keys based on selected model
    model = current_model_settings["model"]
    if model == "gpt-oss":
        # GPT-OSS uses hardcoded Modal endpoint URL (FREE - no API key required!)
        headers["X-Modal-Endpoint-URL"] = current_model_settings["modal_endpoint_url"]
    elif model == "gpt-5" and current_model_settings["openai_api_key"]:
        headers["X-OpenAI-API-Key"] = current_model_settings["openai_api_key"]
    elif model == "grok-4-1" and current_model_settings["xai_api_key"]:
        headers["X-XAI-API-Key"] = current_model_settings["xai_api_key"]
    elif model in ["claude-4.5", "claude-opus"] and current_model_settings["anthropic_api_key"]:
        headers["X-Anthropic-API-Key"] = current_model_settings["anthropic_api_key"]
    elif model == "gemini-3" and current_model_settings["google_api_key"]:
        headers["X-Google-API-Key"] = current_model_settings["google_api_key"]
    
    # Tavily API key for web research (optional - AI model used as fallback)
    if current_model_settings["tavily_api_key"]:
        headers["X-Tavily-API-Key"] = current_model_settings["tavily_api_key"]
    
    return headers

def chat_with_agent_streaming(message: str, history: list, initialized_history: list = None) -> Generator:
    """
    Send a message to the EU AI Act agent and stream the response with tool calls
    
    Args:
        message: User's input message
        history: Original chat history for API (without current user message)
        initialized_history: Pre-initialized history with user message and loading (optional)
        
    Yields:
        Updated history with streaming content
    """
    global cancel_token
    
    if not message.strip():
        yield initialized_history or history
        return
    
    # Reset cancellation token for new request
    cancel_token.reset()
    
    # Use pre-initialized history or create one
    if initialized_history:
        new_history = list(initialized_history)
    else:
        new_history = list(history) + [
            ChatMessage(role="user", content=message),
            ChatMessage(role="assistant", content="‚è≥ *Thinking...*")
        ]
    
    response = None
    bot_response = ""
    tool_calls_content = ""  # All tool calls, results, and thinking sections (in order)
    current_thinking = ""  # Accumulate thinking text before tool calls
    
    try:
        # Convert original history to API format (handle both ChatMessage and dict)
        api_history = []
        for msg in history:
            if isinstance(msg, dict):
                api_history.append({"role": msg.get("role", "user"), "content": msg.get("content", "")})
            else:
                api_history.append({"role": msg.role, "content": msg.content})
        
        # Make streaming request to API with model configuration headers
        response = requests.post(
            f"{API_URL}/api/chat",
            json={"message": message, "history": api_history},
            headers=get_api_headers(),
            stream=True,
            timeout=API_TIMEOUT,
        )
        
        # Register response for potential cancellation
        cancel_token.set_response(response)
        
        if response.status_code != 200:
            error_msg = f"‚ö†Ô∏è Error: API returned status {response.status_code}"
            new_history[-1] = ChatMessage(role="assistant", content=error_msg)
            yield new_history
            return
        
        # Initialize assistant response
        current_tool_call = None
        
        for line in response.iter_lines():
            # Check for cancellation
            if cancel_token.is_cancelled():
                # Include any accumulated thinking before cancellation
                if current_thinking.strip():
                    tool_calls_content += format_thinking_section(current_thinking)
                
                final_content = tool_calls_content + bot_response
                if final_content:
                    final_content += "\n\n*‚Äî Execution stopped by user*"
                else:
                    final_content = "*‚Äî Execution stopped by user*"
                new_history[-1] = ChatMessage(role="assistant", content=final_content)
                yield new_history
                return
            
            if line:
                line_str = line.decode('utf-8')
                print(f"[DEBUG] Received: {line_str[:100]}...")  # Debug log
                if line_str.startswith('data: '):
                    try:
                        data = json.loads(line_str[6:])  # Remove 'data: ' prefix
                        event_type = data.get("type")
                        print(f"[DEBUG] Event type: {event_type}, data: {str(data)[:100]}")
                        
                        if event_type == "thinking":
                            # Handle thinking/reasoning tokens from Claude or GPT
                            # Show thinking at the END (bottom) where action is happening
                            thinking_content = data.get("content", "")
                            if thinking_content:
                                current_thinking += thinking_content
                                
                                # Show thinking tokens in real-time AT THE BOTTOM
                                # Tool calls first, then current thinking at the end
                                live_thinking = f"\n\nüß† **Model Thinking (live):**\n\n```\n{current_thinking}\n```"
                                full_content = tool_calls_content + live_thinking
                                
                                new_history[-1] = ChatMessage(role="assistant", content=full_content)
                                yield new_history
                        
                        elif event_type == "text":
                            # Append text chunk
                            text_content = data.get("content", "")
                            text_phase = data.get("phase", "thinking")  # Server tells us the phase
                            has_had_tools = data.get("hasHadToolCalls", False)
                            
                            # Determine if this is "thinking" text or final response based on server phase
                            if text_phase == "thinking" or (not has_had_tools and not tool_calls_content):
                                # This is thinking text (before tool calls or between them)
                                current_thinking += text_content
                                
                                # Show thinking AT THE BOTTOM after tool calls
                                if not tool_calls_content:
                                    # Initial thinking - show with brain indicator
                                    display_content = f"üß† **AI is reasoning...**\n\n{current_thinking}"
                                else:
                                    # Thinking between tool calls - show at the end
                                    display_content = tool_calls_content + f"\n\nüß† *Reasoning:* {current_thinking}"
                                
                                new_history[-1] = ChatMessage(role="assistant", content=display_content)
                            else:
                                # This is "potential_response" - text after tool results
                                # Could be final response OR thinking before another tool call
                                # We accumulate it and will format appropriately when we know more
                                current_thinking += text_content
                                
                                # Show as streaming response AT THE BOTTOM
                                full_content = tool_calls_content + f"\n\n{current_thinking}"
                                new_history[-1] = ChatMessage(role="assistant", content=full_content)
                            
                            yield new_history
                            
                        elif event_type == "tool_call":
                            # Before showing tool call, save any accumulated thinking as collapsible
                            tool_name = data.get("toolName", "unknown")
                            args = data.get("args", {})
                            
                            # If we have accumulated thinking text, add it as collapsible BEFORE this tool call
                            if current_thinking.strip():
                                tool_calls_content += format_thinking_section(current_thinking, tool_name)
                                current_thinking = ""  # Reset for next thinking block
                            else:
                                # No thinking text was output - add a synthetic thinking note
                                tool_display = tool_name.replace('_', ' ').title()
                                synthetic_thinking = f"I'll use the **{tool_display}** tool to gather the necessary information."
                                tool_calls_content += format_thinking_section(synthetic_thinking, tool_name)
                            
                            # Show tool call with prominent loading indicator AT THE BOTTOM
                            tool_calls_content += format_tool_call(tool_name, args)
                            # Add prominent loading indicator specific to this tool
                            loading_indicator = format_thinking_indicator(tool_name)
                            full_content = tool_calls_content + bot_response + loading_indicator
                            new_history[-1] = ChatMessage(role="assistant", content=full_content)
                            yield new_history
                            current_tool_call = tool_name
                            
                        elif event_type == "tool_result":
                            # Show tool result (removes loading indicator)
                            tool_name = data.get("toolName", current_tool_call or "unknown")
                            result = data.get("result")
                            tool_calls_content += format_tool_result(tool_name, result)
                            
                            # After tool result, show "analyzing results" indicator AT THE BOTTOM
                            analyzing_indicator = f"\n\nüß† **Analyzing {tool_name.replace('_', ' ')} results...**\n"
                            full_content = tool_calls_content + analyzing_indicator
                            new_history[-1] = ChatMessage(role="assistant", content=full_content)
                            yield new_history
                            current_tool_call = None
                            
                        elif event_type == "step_finish":
                            # Step completed - if there's accumulated thinking, add it to tool_calls_content
                            has_had_tools_in_step = data.get("hasHadToolCalls", False)
                            
                            if current_thinking.strip():
                                tool_calls_content += format_thinking_section(current_thinking)
                                current_thinking = ""
                            
                            # Show "preparing response" if we had tool calls and step is finishing AT THE BOTTOM
                            if has_had_tools_in_step and tool_calls_content:
                                preparing_indicator = "\n\n‚ú® **Preparing comprehensive response based on analysis...**\n"
                                full_content = tool_calls_content + preparing_indicator
                            else:
                                full_content = tool_calls_content + bot_response
                            
                            new_history[-1] = ChatMessage(role="assistant", content=full_content)
                            yield new_history
                            
                        elif event_type == "error":
                            error_msg = data.get("error", "Unknown error")
                            bot_response += f"\n\n‚ö†Ô∏è Error: {error_msg}"
                            full_content = tool_calls_content + bot_response
                            new_history[-1] = ChatMessage(role="assistant", content=full_content)
                            yield new_history
                            
                        elif event_type == "done":
                            # Final update
                            # If we have tool calls, any remaining current_thinking is the final response
                            # If no tool calls, current_thinking was just direct response (no tools needed)
                            if tool_calls_content:
                                # We had tool calls - current_thinking after last tool is the final response
                                bot_response = current_thinking
                                current_thinking = ""
                            else:
                                # No tool calls - current_thinking is the direct response
                                bot_response = current_thinking
                                current_thinking = ""
                            
                            # Final response AT THE BOTTOM after all tool calls
                            full_content = tool_calls_content + bot_response
                            new_history[-1] = ChatMessage(role="assistant", content=full_content)
                            yield new_history
                            break
                            
                    except json.JSONDecodeError:
                        continue
        
        # Ensure final state (only if not cancelled)
        if not cancel_token.is_cancelled():
            # If we have accumulated text that wasn't finalized, treat it as the response
            if current_thinking.strip() and not bot_response:
                bot_response = current_thinking
            
            # Final content: tool calls first, then response at the bottom
            final_content = tool_calls_content + (bot_response or "No response generated.")
            new_history[-1] = ChatMessage(role="assistant", content=final_content)
            yield new_history
        
    except requests.exceptions.ConnectionError:
        if not cancel_token.is_cancelled():
            error_msg = "‚ö†Ô∏è Cannot connect to API server. Make sure it's running on http://localhost:3001"
            new_history[-1] = ChatMessage(role="assistant", content=error_msg)
            yield new_history
    except requests.exceptions.Timeout:
        if not cancel_token.is_cancelled():
            error_msg = "‚ö†Ô∏è Request timed out. The agent might be processing a complex query."
            final_content = tool_calls_content + bot_response + "\n\n" + error_msg
            new_history[-1] = ChatMessage(role="assistant", content=final_content)
            yield new_history
    except (requests.exceptions.ChunkedEncodingError, ConnectionError):
        # This can happen when we close the connection during cancellation - it's expected
        if not cancel_token.is_cancelled():
            error_msg = "‚ö†Ô∏è Connection was interrupted."
            final_content = tool_calls_content + bot_response + "\n\n" + error_msg
            new_history[-1] = ChatMessage(role="assistant", content=final_content)
            yield new_history
    except Exception as e:
        if not cancel_token.is_cancelled():
            error_msg = f"‚ö†Ô∏è Error: {str(e)}"
            final_content = tool_calls_content + bot_response + "\n\n" + error_msg if (tool_calls_content or bot_response) else error_msg
            new_history[-1] = ChatMessage(role="assistant", content=final_content)
            yield new_history
    finally:
        # Clean up the response connection
        if response is not None:
            try:
                response.close()
            except:
                pass
        cancel_token.set_response(None)

def check_api_status() -> str:
    """Check if the API server is running"""
    try:
        response = requests.get(f"{API_URL}/health", timeout=5)
        if response.status_code == 200:
            data = response.json()
            return f"‚úÖ API Server: {data.get('service')} v{data.get('version')}"
        else:
            return f"‚ö†Ô∏è API Server returned status {response.status_code}"
    except requests.exceptions.ConnectionError:
        return "‚ùå API Server not running. Start it with: pnpm dev"
    except Exception as e:
        return f"‚ùå Error: {str(e)}"

def get_available_tools() -> str:
    """Get list of available MCP tools with descriptions"""
    try:
        response = requests.get(f"{API_URL}/api/tools", timeout=5)
        if response.status_code == 200:
            data = response.json()
            tools = data.get("tools", [])
            if tools:
                tool_list = "\n".join([f"‚Ä¢ **{t['name']}**" for t in tools])
                return f"""**Available MCP Tools:**

{tool_list}

**‚ú® Capabilities:**
‚Ä¢ Generate complete compliance reports
‚Ä¢ Create documentation templates (Risk Management, Technical Docs, etc.)
‚Ä¢ Discover AI systems and assess risk levels
‚Ä¢ Analyze organization compliance gaps"""
            return "No tools available"
        return "Could not fetch tools"
    except:
        return "Could not connect to API"

def get_example_queries() -> List[List[str]]:
    """Get example queries for the interface"""
    return [
        # MCP Tools Examples - Showcase full compliance analysis capabilities
        ["Generate a complete EU AI Act compliance report for Microsoft with all documentation templates"],
        ["Analyze IBM's watsonX system compliance and generate risk management documentation"],
        ["Create full compliance assessment for OpenAI including technical documentation templates"],
        # General Questions
        ["What is the EU AI Act?"],
        ["Is a recruitment screening AI considered high-risk?"],
        ["What are the compliance requirements for chatbots?"],
        ["What's the timeline for EU AI Act enforcement?"],
    ]

# Default browser state for persistent storage
DEFAULT_BROWSER_STATE = {
    "api_keys": {
        "tavily": "",
        "anthropic": "",
        "google": "",
        "openai": "",
        "xai": ""
    },
    "model": "gpt-oss"
}

# Create Gradio interface
with gr.Blocks(
    title="üá™üá∫ EU AI Act Compliance Agent",
) as demo:
    # Browser state for persistent storage (persists across page refreshes)
    browser_state = gr.BrowserState(DEFAULT_BROWSER_STATE)
    
    # Custom CSS only - JavaScript is loaded via gr.Blocks(js=...) parameter
    gr.HTML("""
    <style>
    /* Hide Gradio's default footer */
    footer { display: none !important; }
    .gradio-container footer { display: none !important; }
    .footer { display: none !important; }
    [data-testid="footer"] { display: none !important; }
    
    /* Style the stop button */
    button.stop {
        background-color: #dc3545 !important;
        border-color: #dc3545 !important;
    }
    button.stop:hover {
        background-color: #c82333 !important;
        border-color: #bd2130 !important;
    }
    
    /* Scroll indicator when user has scrolled up */
    .scroll-indicator {
        position: absolute;
        bottom: 10px;
        right: 20px;
        background: rgba(0, 0, 0, 0.7);
        color: white;
        padding: 8px 16px;
        border-radius: 20px;
        font-size: 12px;
        cursor: pointer;
        z-index: 1000;
        display: none;
    }
    .scroll-indicator:hover {
        background: rgba(0, 0, 0, 0.9);
    }
    
    /* Keys loaded indicator */
    .keys-loaded-badge {
        display: inline-block;
        background: #28a745;
        color: white;
        padding: 2px 8px;
        border-radius: 12px;
        font-size: 11px;
        margin-left: 8px;
    }
    </style>
    """)
    
    # Header - use PUBLIC_URL for production links
    # In production (HF Spaces): Show info about gradio.live URL
    # In local dev: Direct link to localhost:7861
    chatgpt_link_href = PUBLIC_URL if PUBLIC_URL else "http://localhost:7861"
    is_production = bool(PUBLIC_URL)
    
    # Get MCP URL (written by chatgpt_app.py when it starts)
    mcp_url = get_mcp_url()
    
    # MCP Server is deployed separately
    MCP_SPACE_URL = "https://mcp-1st-birthday-eu-ai-act-chatgpt-mcp.hf.space"
    MCP_URL = f"{MCP_SPACE_URL}/gradio_api/mcp/"
    
    if is_production:
        # Production: Link to separate MCP Space
        chatgpt_section = f"""
            <a href="{MCP_SPACE_URL}" target="_blank" style="text-decoration: none;">
                <div style="background: linear-gradient(135deg, #4CAF50 0%, #45a049 100%); padding: 12px 20px; border-radius: 10px; display: inline-block;">
                    <span style="color: #fff; font-size: 0.9em;">
                        üí¨ <strong>ChatGPT MCP Server</strong>
                    </span>
                    <p style="color: rgba(255,255,255,0.9); font-size: 0.75em; margin: 8px 0 0 0; word-break: break-all;">
                        <code style="background: rgba(255,255,255,0.2); padding: 3px 6px; border-radius: 3px;">{MCP_URL}</code>
                    </p>
                    <p style="color: rgba(255,255,255,0.7); font-size: 0.7em; margin: 6px 0 0 0;">
                        Click to open MCP Server Space ‚Üí
                    </p>
                </div>
            </a>
        """
    else:
        # Local dev: Direct link
        chatgpt_section = """
            <a href="http://localhost:7861" target="_blank" style="color: #2196F3; text-decoration: none; padding: 6px 12px; border: 1px solid #2196F3; border-radius: 4px; display: inline-block;">
                üí¨ Open ChatGPT App (MCP Server)
            </a>
        """
    
    gr.HTML(f"""
        <div style="text-align: center; padding: 20px 0;">
            <h1 style="margin: 0; font-size: 2em;">üá™üá∫ EU AI Act Compliance Agent</h1>
            <p style="margin: 10px 0 0 0; opacity: 0.8;">by <a href="https://www.legitima.ai/mcp-hackathon" target="_blank" style="color: #4CAF50;">Legitima.ai</a></p>
            <p style="margin: 5px 0; font-size: 0.9em; opacity: 0.7;">Your intelligent assistant for navigating European AI regulation</p>
            <p style="margin: 10px 0 0 0; font-size: 0.9em;">
                {chatgpt_section}
            </p>
        </div>
    """)
    
    # Main content
    with gr.Row():
        with gr.Column(scale=3):
            # Chat interface - using ChatMessage format
            chatbot = gr.Chatbot(
                label="Chat with EU AI Act Expert",
                height=550,
                show_label=True,
                autoscroll=False,  # Disable auto-scroll - we handle it with JS
            )
            
            with gr.Row():
                msg = gr.Textbox(
                    placeholder="Ask about compliance, or request a full compliance report with documentation for an organization...",
                    show_label=False,
                    scale=8,
                )
                submit = gr.Button("Send", variant="primary", scale=1)
                stop_btn = gr.Button("‚èπ Stop", variant="stop", scale=1, visible=False)
            
            gr.Examples(
                examples=get_example_queries(),
                inputs=msg,
                label="üí° Example Questions (Try MCP tools for compliance reports & documentation!)",
            )
        
        with gr.Column(scale=1):
            # Sidebar
            gr.Markdown("### ü§ñ Model Settings")
            
            model_dropdown = gr.Dropdown(
                choices=[(v["name"], k) for k, v in AVAILABLE_MODELS.items()],
                value=current_model_settings["model"],  # Use current model setting
                label="AI Model",
                info="Select the AI model to use. ‚ö†Ô∏è GPT-OSS is FREE but may take up to 60s to start (cold start). For faster responses and better precision, use another model with your API key.",
                elem_id="model_dropdown"
            )
            
            # API Key inputs (password fields) - GPT-OSS is FREE, other models require API keys
            with gr.Accordion("üîë API Keys & Settings", open=True):
                gr.Markdown("""üÜì **GPT-OSS 20B is FREE** - Uses pre-configured Modal endpoint (no setup required).

‚è±Ô∏è **Note:** GPT-OSS may take up to **60 seconds** to start responding due to cold start. For **faster responses and better precision**, select another model and provide your API key below.

‚ö†Ô∏è For paid models (Claude, GPT-5, Gemini, Grok), an API key is required.

üîê Keys are stored securely in encoded cookies and **auto-expire after 24 hours**.

‚ÑπÔ∏è *Tavily is **optional** - enhances web research for organization & AI systems discovery. Falls back to server's `TAVILY_API_KEY` env var if not provided, then to AI model.*""")
                
                gr.Markdown("#### üîç Research API (Optional)")
                tavily_key = gr.Textbox(
                    label="Tavily API Key (Optional)",
                    placeholder="tvly-... (optional - enhances web research)",
                    type="password",
                    value="",  # Will be populated from cookies via JS
                    info="Optional - uses server env var fallback, then AI model.",
                    elem_id="tavily_key_input"
                )
                
                gr.Markdown("#### ü§ñ AI Model APIs")
                anthropic_key = gr.Textbox(
                    label="Anthropic API Key *",
                    placeholder="sk-ant-... (required for Claude models)",
                    type="password",
                    value="",  # Will be populated from cookies via JS
                    info="Required - for Claude 4.5 Sonnet or Claude Opus 4",
                    elem_id="anthropic_key_input"
                )
                google_key = gr.Textbox(
                    label="Google API Key",
                    placeholder="AIza... (required for Gemini 3)",
                    type="password",
                    value="",  # Will be populated from cookies via JS
                    info="Required if using Gemini 3 Pro model",
                    elem_id="google_key_input"
                )
                openai_key = gr.Textbox(
                    label="OpenAI API Key",
                    placeholder="sk-... (required for GPT-5)",
                    type="password",
                    value="",  # Will be populated from cookies via JS
                    info="Required if using GPT-5 model",
                    elem_id="openai_key_input"
                )
                xai_key = gr.Textbox(
                    label="xAI API Key",
                    placeholder="xai-... (required for Grok 4.1)",
                    type="password",
                    value="",  # Will be populated from cookies via JS
                    info="Required if using Grok 4.1 model",
                    elem_id="xai_key_input"
                )
                with gr.Row():
                    save_keys_btn = gr.Button("üíæ Save Keys", variant="secondary", size="sm")
                    clear_keys_btn = gr.Button("üóëÔ∏è Clear Keys", variant="stop", size="sm")
                keys_status = gr.Markdown("")
            
            gr.Markdown("---")
            
            gr.Markdown("### üìä Quick Reference")
            
            gr.Markdown("""
**Risk Categories:**
- üî¥ **Unacceptable** - Banned
- üü† **High Risk** - Strict requirements
- üü° **Limited Risk** - Transparency
- üü¢ **Minimal Risk** - No obligations

**Key Deadlines:**
- üìÖ Feb 2, 2025: Banned AI
- üìÖ Aug 2, 2026: High-risk rules
- üìÖ Aug 2, 2027: Full enforcement
            """)
            
            gr.Markdown("---")
            
            tools_info = gr.Markdown(
                value=get_available_tools(),
                label="üîß MCP Tools - Generate Reports & Documentation"
            )
            
            gr.Markdown("---")
            
            # Sidebar ChatGPT App section
            if is_production:
                sidebar_chatgpt = f"""
                <a href="{MCP_SPACE_URL}" target="_blank" style="text-decoration: none;">
                    <div style="background: linear-gradient(135deg, #4CAF50 0%, #45a049 100%); padding: 12px; border-radius: 8px; margin: 5px 0;">
                        <strong style="color: #fff;">üí¨ MCP Server</strong>
                        <p style="color: rgba(255,255,255,0.9); font-size: 0.7em; margin: 6px 0 0 0;">
                            Click to get MCP URL ‚Üí
                        </p>
                    </div>
                </a>
                """
            else:
                sidebar_chatgpt = """
                <a href="http://localhost:7861" target="_blank" style="color: #2196F3; text-decoration: none; padding: 8px 16px; border: 1px solid #2196F3; border-radius: 6px; display: inline-block; margin: 5px 0;">
                    üí¨ ChatGPT App (MCP Server)
                </a>
                <p style="font-size: 0.85em; opacity: 0.7; margin-top: 8px;">
                    Use the ChatGPT App to connect with ChatGPT Desktop and access MCP tools via OpenAI Apps SDK.
                </p>
                """
            
            gr.HTML(f"""
            <div>
                <h3 style="margin-bottom: 10px;">üîó Other Interfaces</h3>
                <div>
                    {sidebar_chatgpt}
                </div>
            </div>
            """)
            
            gr.Markdown("---")
            
            status = gr.Textbox(
                label="üîå API Status",
                value=check_api_status(),
                interactive=False,
                max_lines=2,
            )
            
            with gr.Row():
                refresh_btn = gr.Button("üîÑ Refresh", size="sm")
                clear_btn = gr.Button("üóëÔ∏è Clear", size="sm")
    
    # Footer
    gr.Markdown("""
---
<div style="text-align: center; opacity: 0.7; font-size: 0.85em;">
    <p>Built for the MCP 1st Birthday Hackathon üéÇ</p>
    <p>Powered by Vercel AI SDK v5 + Model Context Protocol + Gradio</p>
</div>
    """)
    
    # Disclaimer box - separate for better visibility
    gr.HTML("""
<style>
.disclaimer-box {
    text-align: center;
    margin: 20px auto;
    padding: 15px 20px;
    max-width: 800px;
    background: #fff3cd !important;
    border: 2px solid #ffc107 !important;
    border-radius: 8px;
    box-shadow: 0 2px 4px rgba(0,0,0,0.1);
}
.disclaimer-box p {
    color: #000000 !important;
    margin: 0;
}
.disclaimer-box strong {
    color: #000000 !important;
}
</style>
<div class="disclaimer-box">
    <p style="font-size: 0.9em; font-weight: 500; margin-bottom: 8px;">
        <strong>‚ö†Ô∏è Disclaimer:</strong> This is a <strong style="background: rgba(255, 193, 7, 0.3); padding: 2px 4px; border-radius: 3px;">demo application (Work in Progress)</strong> and does not constitute legal advice.
    </p>
    <p style="font-size: 0.85em; line-height: 1.4;">
        Always consult with qualified legal professionals before making compliance decisions based on AI outputs.
    </p>
</div>
    """)
    
    # Model and API key handlers
    def update_model(model_value):
        """Update the selected model"""
        current_model_settings["model"] = model_value
        model_info = AVAILABLE_MODELS.get(model_value, {})
        print(f"[Gradio] Model updated to: {model_value} ({model_info.get('name', model_value)})")
        return f"‚úÖ Model set to: **{model_info.get('name', model_value)}**"
    
    def save_api_keys(tavily_val, anthropic_val, google_val, openai_val, xai_val):
        """Save user-provided API keys to session AND secure cookie storage
        
        SECURITY: Keys are stored in memory for this session AND in encoded cookies
        that expire after 1 day. Cookies use XOR obfuscation + base64 encoding.
        GPT-OSS is FREE and uses pre-configured Modal endpoint from environment.
        Paid models require API keys. Note: Tavily is OPTIONAL - AI model is used as fallback for research.
        """
        saved = []
        
        # Only update if a real key is provided
        if tavily_val and len(tavily_val) > 10:
            current_model_settings["tavily_api_key"] = tavily_val
            saved.append("Tavily")
        
        if anthropic_val and len(anthropic_val) > 10:
            current_model_settings["anthropic_api_key"] = anthropic_val
            saved.append("Anthropic")
        
        if google_val and len(google_val) > 10:
            current_model_settings["google_api_key"] = google_val
            saved.append("Google")
        
        if openai_val and len(openai_val) > 10:
            current_model_settings["openai_api_key"] = openai_val
            saved.append("OpenAI")
        
        if xai_val and len(xai_val) > 10:
            current_model_settings["xai_api_key"] = xai_val
            saved.append("xAI")
        
        # Build status message
        status_parts = []
        
        # Always show current model
        model = current_model_settings["model"]
        model_info = AVAILABLE_MODELS.get(model, {})
        status_parts.append(f"ü§ñ **Model:** {model_info.get('name', model)}")
        
        if saved:
            status_parts.append(f"‚úÖ **Keys Saved:** {', '.join(saved)}")
        
        status_parts.append("üîê *Settings stored in secure cookies (expires in 24h)*")
        
        # Check for missing required keys based on selected model
        if model == "gpt-oss":
            # GPT-OSS uses hardcoded Modal endpoint - always available
            status_parts.append("üÜì *GPT-OSS model is FREE - no API key required!*")
        elif model in ["claude-4.5", "claude-opus"] and not current_model_settings["anthropic_api_key"]:
            status_parts.append(f"‚ö†Ô∏è **Anthropic API key required** for {model}")
        elif model == "gemini-3" and not current_model_settings["google_api_key"]:
            status_parts.append("‚ö†Ô∏è **Google API key required** for Gemini 3")
        elif model == "gpt-5" and not current_model_settings["openai_api_key"]:
            status_parts.append("‚ö†Ô∏è **OpenAI API key required** for GPT-5")
        elif model == "grok-4-1" and not current_model_settings["xai_api_key"]:
            status_parts.append("‚ö†Ô∏è **xAI API key required** for Grok 4.1")
        
        # Tavily is optional - just inform user about enhanced features if they have it
        if not current_model_settings["tavily_api_key"]:
            status_parts.append("‚ÑπÔ∏è *Tavily not set - will use server env var fallback or AI model*")
        
        return "\n\n".join(status_parts)
    
    def get_current_model_status():
        """Get current model and key status"""
        model = current_model_settings["model"]
        model_info = AVAILABLE_MODELS.get(model, {})
        required_key = model_info.get("api_key_env", "")
        
        key_status = "‚ùå Missing"
        if required_key == "OPENAI_API_KEY" and current_model_settings["openai_api_key"]:
            key_status = "‚úÖ Set"
        elif required_key == "XAI_API_KEY" and current_model_settings["xai_api_key"]:
            key_status = "‚úÖ Set"
        elif required_key == "ANTHROPIC_API_KEY" and current_model_settings["anthropic_api_key"]:
            key_status = "‚úÖ Set"
        
        return f"**Model:** {model_info.get('name', model)}\n**Key Status:** {key_status}"
    
    def check_required_keys():
        """Check if required API keys are configured
        
        Returns a tuple of (is_valid, error_message)
        - is_valid: True if all required keys are present
        - error_message: Description of missing keys if not valid
        
        Note: GPT-OSS is FREE and no API key is required.
        Tavily API key is optional - the system will fallback to AI model for research.
        """
        missing_keys = []
        
        # Check model API key based on selected model
        model = current_model_settings["model"]
        if model == "gpt-oss":
            # GPT-OSS uses hardcoded endpoint - no check needed
            pass
        elif model in ["claude-4.5", "claude-opus"] and not current_model_settings["anthropic_api_key"]:
            missing_keys.append(f"**Anthropic API Key** (required for {model})")
        elif model == "gemini-3" and not current_model_settings["google_api_key"]:
            missing_keys.append("**Google API Key** (required for Gemini 3)")
        elif model == "gpt-5" and not current_model_settings["openai_api_key"]:
            missing_keys.append("**OpenAI API Key** (required for GPT-5)")
        elif model == "grok-4-1" and not current_model_settings["xai_api_key"]:
            missing_keys.append("**xAI API Key** (required for Grok 4.1)")
        
        # Note: Tavily is OPTIONAL - system will fallback to AI model for research
        # We no longer require Tavily API key
        
        if missing_keys:
            error_msg = """## ‚ö†Ô∏è API Keys Required

To use this service, you need to provide your own API keys. The following keys are missing:

"""
            for key in missing_keys:
                error_msg += f"- {key}\n"
            
            error_msg += """
### How to add your API keys:

1. Expand the **üîë API Keys & Settings** section in the sidebar
2. Enter your API keys in the corresponding fields
3. Click **üíæ Save Keys**

### Where to get API keys:

- **Anthropic**: [console.anthropic.com](https://console.anthropic.com) - Get Claude API key
- **Google**: [aistudio.google.com](https://aistudio.google.com/apikey) - Get Gemini API key
- **OpenAI**: [platform.openai.com](https://platform.openai.com) - Get GPT API key
- **xAI**: [console.x.ai](https://console.x.ai) - Get Grok API key

**üÜì FREE Alternative:**
- Select **GPT-OSS 20B** from the model dropdown - it's FREE via Modal.com!

**Optional:**
- **Tavily**: [tavily.com](https://tavily.com) - For enhanced web research (falls back to server env var, then AI model)
"""
            return False, error_msg
        
        return True, ""
    
    # Event handlers - clear input immediately and stream response together
    def respond_and_clear(message: str, history: list):
        """Wrapper that yields (cleared_input, chat_history, stop_visible) tuples"""
        global cancel_token
        
        if not message.strip():
            yield "", history, gr.update(visible=False)
            return
        
        # Check for required API keys before proceeding
        keys_valid, error_message = check_required_keys()
        if not keys_valid:
            # Show user message and error about missing keys
            error_history = list(history) + [
                ChatMessage(role="user", content=message),
                ChatMessage(role="assistant", content=error_message)
            ]
            yield "", error_history, gr.update(visible=False)
            return
        
        # Reset cancellation token for new request
        cancel_token.reset()
            
        # First yield: clear input, show loading, and show stop button
        # Get the actual selected model from current_model_settings (not from env)
        selected_model = current_model_settings.get("model", "gpt-oss")
        model_info = AVAILABLE_MODELS.get(selected_model, {})
        model_name = model_info.get("name", selected_model)
        print(f"[Gradio] Using model: {selected_model} ({model_name})")
        initial_history = list(history) + [
            ChatMessage(role="user", content=message),
            ChatMessage(role="assistant", content=f"‚è≥ *Thinking with {model_name}...*")
        ]
        yield "", initial_history, gr.update(visible=True)
        
        # Stream the actual response (pass initialized_history to avoid duplication)
        updated_history = initial_history  # Initialize in case generator doesn't yield
        for updated_history in chat_with_agent_streaming(message, history, initial_history):
            # Check if cancelled during streaming
            if cancel_token.is_cancelled():
                break
            yield "", updated_history, gr.update(visible=True)
        
        # Final yield: hide stop button when done
        yield "", updated_history, gr.update(visible=False)
    
    def stop_response(history: list):
        """Stop the current response by triggering cancellation"""
        global cancel_token
        
        # Trigger cancellation - this will close the HTTP connection
        cancel_token.cancel()
        
        # Update history to show stopped state (the generator will also update when it detects cancellation)
        if history and len(history) > 0:
            last_msg = history[-1]
            if isinstance(last_msg, ChatMessage):
                content = last_msg.content
                # Remove thinking indicator and add stopped message
                if "‚è≥" in content:
                    content = content.replace("‚è≥ *Thinking", "‚èπÔ∏è *Stopped")
                    content = content.replace("‚è≥ *Processing", "‚èπÔ∏è *Stopped")
                if "*‚Äî Execution stopped by user*" not in content:
                    content += "\n\n*‚Äî Execution stopped by user*"
                history[-1] = ChatMessage(role="assistant", content=content)
        
        # Return history and hide stop button
        return history, gr.update(visible=False)
    
    # On submit/click: clear input immediately while streaming response
    # These events are cancellable by the stop button
    submit_event = msg.submit(respond_and_clear, [msg, chatbot], [msg, chatbot, stop_btn])
    click_event = submit.click(respond_and_clear, [msg, chatbot], [msg, chatbot, stop_btn])
    
    # Stop button cancels the streaming events and updates the chat
    stop_btn.click(
        fn=stop_response,
        inputs=[chatbot],
        outputs=[chatbot, stop_btn],
        cancels=[submit_event, click_event]
    )
    
    refresh_btn.click(
        lambda: (check_api_status(), get_available_tools()), 
        None, 
        [status, tools_info]
    )
    clear_btn.click(lambda: [], None, chatbot)
    
    # Function to update model in browser state
    def save_model_to_browser_state(model_val, stored_data):
        """Save model selection to browser state"""
        if stored_data is None:
            stored_data = DEFAULT_BROWSER_STATE.copy()
        new_data = stored_data.copy()
        new_data["model"] = model_val or "gpt-oss"
        print(f"[BrowserState] Model changed to: {model_val}")
        return new_data
    
    # Model selection handler - also saves to browser state
    model_dropdown.change(
        update_model, 
        [model_dropdown], 
        [keys_status]
    ).then(
        save_model_to_browser_state,
        [model_dropdown, browser_state],
        [browser_state]
    )
    
    # Function to save settings to browser state
    def save_to_browser_state(tavily_val, anthropic_val, google_val, openai_val, xai_val, model_val, stored_data):
        """Save API keys and model to browser state (persists across refreshes)"""
        new_data = {
            "api_keys": {
                "tavily": tavily_val or "",
                "anthropic": anthropic_val or "",
                "google": google_val or "",
                "openai": openai_val or "",
                "xai": xai_val or ""
            },
            "model": model_val or "gpt-oss"
        }
        print(f"[BrowserState] Saving to browser: model={model_val}, keys={[k for k,v in new_data['api_keys'].items() if v]}")
        return new_data
    
    # Combined save handler - saves to session AND browser state
    save_keys_btn.click(
        save_api_keys, 
        [tavily_key, anthropic_key, google_key, openai_key, xai_key], 
        [keys_status]
    ).then(
        save_to_browser_state,
        [tavily_key, anthropic_key, google_key, openai_key, xai_key, model_dropdown, browser_state],
        [browser_state]
    )
    
    # Clear keys function - clears both session and cookies, resets model to default
    def clear_api_keys():
        """Clear all stored API keys from session and cookies, reset model to default"""
        # Note: modal_endpoint_url is hardcoded, so we don't clear it
        current_model_settings["tavily_api_key"] = ""
        current_model_settings["anthropic_api_key"] = ""
        current_model_settings["google_api_key"] = ""
        current_model_settings["openai_api_key"] = ""
        current_model_settings["xai_api_key"] = ""
        current_model_settings["model"] = "gpt-oss"  # Reset to default FREE model
        # Return: tavily, anthropic, google, openai, xai, model_value, status
        return "", "", "", "", "", "gpt-oss", "üóëÔ∏è All settings cleared (model reset to GPT-OSS)"
    
    # Function to clear browser state
    def clear_browser_state():
        """Clear all stored data from browser state"""
        print("[BrowserState] Clearing all stored data")
        return DEFAULT_BROWSER_STATE
    
    # Combined clear handler - clears session AND browser state
    clear_keys_btn.click(
        clear_api_keys,
        [],
        [tavily_key, anthropic_key, google_key, openai_key, xai_key, model_dropdown, keys_status]
    ).then(
        clear_browser_state,
        [],
        [browser_state]
    )
    
    # === Load handler: Restore API keys and model from browser storage on page load ===
    def load_from_browser_state(stored_data):
        """Load API keys and model from browser storage (runs on page load)
        
        Returns: (tavily, anthropic, google, openai, xai, model, status_message)
        """
        if not stored_data:
            print("[BrowserState] No stored data found")
            return "", "", "", "", "", "gpt-oss", "üîß Ready - configure API keys to get started"
        
        api_keys = stored_data.get("api_keys", {})
        model = stored_data.get("model", "gpt-oss")
        
        # Extract individual keys
        tavily = api_keys.get("tavily", "")
        anthropic = api_keys.get("anthropic", "")
        google = api_keys.get("google", "")
        openai = api_keys.get("openai", "")
        xai = api_keys.get("xai", "")
        
        # Check which keys were loaded
        loaded_keys = [k for k, v in api_keys.items() if v]
        
        if loaded_keys or model != "gpt-oss":
            print(f"[BrowserState] Restoring: model={model}, keys={loaded_keys}")
            
            # Also update the session state
            current_model_settings["model"] = model
            if tavily:
                current_model_settings["tavily_api_key"] = tavily
            if anthropic:
                current_model_settings["anthropic_api_key"] = anthropic
            if google:
                current_model_settings["google_api_key"] = google
            if openai:
                current_model_settings["openai_api_key"] = openai
            if xai:
                current_model_settings["xai_api_key"] = xai
            
            # Build status message
            model_info = AVAILABLE_MODELS.get(model, {})
            status_parts = []
            status_parts.append(f"ü§ñ **Model:** {model_info.get('name', model)}")
            
            if loaded_keys:
                status_parts.append(f"üîê **Restored from browser:** {', '.join(loaded_keys)}")
            
            if model == "gpt-oss":
                status_parts.append("üÜì *GPT-OSS model is FREE - no API key required!*")
            
            status = "\n\n".join(status_parts)
        else:
            print("[BrowserState] No saved settings to restore")
            status = "üîß Ready - configure API keys to get started"
        
        return tavily, anthropic, google, openai, xai, model, status
    
    # Trigger on page load - restore saved settings from browser storage
    demo.load(
        load_from_browser_state,
        inputs=[browser_state],
        outputs=[tavily_key, anthropic_key, google_key, openai_key, xai_key, model_dropdown, keys_status]
    )

# Launch the app
if __name__ == "__main__":
    print("\n" + "="*60)
    print("üá™üá∫ EU AI Act Compliance Agent - Gradio UI")
    print("="*60)
    print(f"\nüì° API Server: {API_URL}")
    print(f"‚úì Status: {check_api_status()}")
    print(f"\nüöÄ Starting Gradio interface...")
    print("="*60 + "\n")
    
    demo.launch(
        server_name=os.getenv("GRADIO_SERVER_NAME", "0.0.0.0"),
        server_port=int(os.getenv("GRADIO_SERVER_PORT", "7860")),
        share=os.getenv("GRADIO_SHARE", "false").lower() == "true",
        show_error=True,
    )
